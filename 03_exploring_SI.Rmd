---
title: "exploring_SI"
output:  
  github_document:
  pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=T, error=F, message=F, warning=F)
```

This file explores the data provided by Gardner et al. (MS, PLoS).

Libraries
```{r libraries}
library(dplyr)
library(ggplot2)
library(mgcv)
library(mgcViz)
library(lmerTest)
library(effects)
library(MASS)
library(rmcorr)
library(pwr)
```

Load the data.
```{r load_data}
dat = read.table("./02_supp_inf.txt", header=T, sep="\t", comment.char="", quote="")

# Some housekeeping
dat$Speaker_Number = as.factor(dat$Speaker_Number)

# We must compute normalized values ourselves (as these columns are not provided, which introduces another degree of freedom in comparing the results here to those reported).

# Function to normalize counts per 100 words
norm.fnc = function(var.ct, samp.size, norm.factor=100){
    prop = var.ct/samp.size
    norm.val = prop*norm.factor
    return(norm.val)
}

# Compute for total values: variable contexts, disfluencies and seconds.silence
dat$Norm.Variable.Contexts = norm.fnc(dat$Total.Variable.Contexts, dat$Total.Words)
dat$Norm.Overt.Disfluencies = norm.fnc(dat$Total.Overt.Disfluencies, dat$Total.Words)
dat$Norm.Seconds.Silence = norm.fnc(dat$Total.Seconds.Silence, dat$Total.Words)

```

Now, just check some of the figures in the text.
  - 190 transcripts
  - 9065 variable contexts
  - 34 speakers
  
```{r check_figures}
# number of transcripts
cat(paste0("Number of transcripts = ", length(unique(dat$Audiofile)), "\n"))

# number of variable contexts
cat(paste0("Number of var. contexts from table = ", sum(dat$Total.Variable.Contexts), "\n"))

## Trying again, this time by computing the measures myself from the other columns
cols = 9:28
dat$CustomVarCon = apply(dat[, cols], 1, sum)
cat(paste0("Number of var. contexts from row sums = ", sum(dat$CustomVarCon), "\n"))

## and just to prove it:
cor.test(dat$Total.Variable.Contexts, dat$CustomVarCon) # identical

# number of speakers
cat(paste0("Number of speakers = ", length(unique(dat$Speaker_Number)), "\n"))
```

For some reason, I am not able to replicate the numbers reported in the text. Perhaps the data had been pruned in a way I didn't catch? I can only assume that each speaker is from the same category (i.e., no males, as reported in the text), so that the "Dyad" column does not combine with the "Speaker_Side" variable to indicate sex (i.e., it is not the case that if Speaker_Side = B and Dyad = Male, then the speaker is male).

I find fewer unique files than reported (183 here vs. 190 reported) as well as a much lower number of variable contexts (6,753 vs. 9,065). But this appears to be the true value for the table as reflected by the row sums. 

Finally, I get fewer unique speakers than reported (30 vs. 34).

So we take a look at the basic properties of the data.

First, sizes of conversations. One would expect that the size of the conversations dictates both the number of disfluencies, as well as the number of opportunities for variabile structures to surface. 

```{r desc_stats}
# Size of individual conversations
## Histogram
conv.size.hist = ggplot(dat, aes(Total.Words)) +
                 geom_histogram(aes(y = ..density..), color="darkblue", fill="white") +
                 geom_density(alpha=.2, fill="darkred") +
                 xlab("Number of words") +
                 ggtitle("Distribution of conversation lengths") +
                 theme_bw() +
                 theme(plot.title = element_text(hjust = 0.5))

conv.size.hist

## ECDF plot
conv.size.ecdf = ggplot(dat, aes(Total.Words)) + 
                 stat_ecdf(geom = "step") +
                 xlab("Number of words") +
                 ylab("%") +
                 ggtitle("Distribution of conversation lengths") +
                 theme_bw() + 
                 theme(plot.title = element_text(hjust = 0.5))
                

conv.size.ecdf
```

The bulk of the conversations contain fewer than 1500 words, and 3/4 of them contain fewer than 1000. Thus, these samples are relatively small.

How many data points does each speaker produce?
```{r how_many_points}
tab = table(dat$Speaker_Number); sort(tab, decreasing=T)

speaker.freqs = data.frame(speaker = names(tab), freq = as.numeric(tab))

spkr.plot = ggplot(speaker.freqs, aes(freq)) +               
            geom_histogram(aes(y = ..density..), color="darkblue", fill="white") +
            geom_density(alpha=.2, fill="darkred") +
            xlab("Number of conversations") +
            ggtitle("Distribution of conversations per speaker") +
            theme_bw() +
            theme(plot.title = element_text(hjust = 0.5))

spkr.plot

# Save speakers with >= 5 conversations
freq.spkrs = speaker.freqs %>%
             filter(freq >= 5)

freq.spkrs = pull(freq.spkrs, speaker)

```

Five speakers only contribute a single data point to the analysis. This makes it difficult to assess how they would behave across multiple contexts. But another point of concern is the relatively small number of observations per participant. Recent work on the "replicability crisis" has revealed that such low numbers -- without expectations of huge effect sizes (and/or proper power analyses) -- results in estimates (e.g., of correlation) that are (a) overestimated in terms of absolute magnitude, (b) susceptible to flips in direction of effects depending on the sample, and (c) at greater risk of both Type I and Type II error (see the work of Vasishth and colleagues). The authors should carefully consider this in their discussion and motivation of the methods they employ. 

As is typical with corpus data, we are dealing with unbalanced samples. Being the most conservative, I plot our expectations for simple correlations given the largest sample size in your dataset across three expected effect sizes with alpha = 0.5.

```{r power_analysis}

# NB: I take this estimation of power from the paper which introduces rmcorr (Bakdash & Marusich, 2017): (N*(k-1)) + 1; N = # of participants, k = average number of observations

power.rmcorr = function(k, N, effectsizer, sig){
    pwr.r.test(n = ((N)*(k-1))+1, r = effectsizer, sig.level = sig) 
}

plot(density(speaker.freqs$freq)) # not normally distributed, so mean may not be the best
k.mean = mean(speaker.freqs$freq)
k.med = median(speaker.freqs$freq)

## Means
# Small effect
power.rmcorr(k.mean, 30, .1, .05)$power

# Medium effect
power.rmcorr(k.mean, 30, .3, .05)$power

# Large effect
power.rmcorr(k.mean, 30, .5, .05)$power

## Median
# Small effect
power.rmcorr(k.med, 30, .1, .05)$power

# Medium effect
power.rmcorr(k.med, 30, .3, .05)$power

# Large effect
power.rmcorr(k.med, 30, .5, .05)$power

effect.sizes = seq(0.1, .5, .05)
vals.med = vector()
vals.mean = vector()
for(es in effect.sizes){
    curr.pwr.med = power.rmcorr(k.med, 30, es, .05)$power
    curr.pwr.mean = power.rmcorr(k.mean, 30, es, .05)$power
    vals.med = c(vals.med, curr.pwr.med)
    vals.mean = c(vals.mean, curr.pwr.mean)
}

vals = data.frame(Power = c(vals.mean, vals.med), k.type = rep(c("mean", "median"), each = length(effect.sizes)), effect.size = rep(effect.sizes, 2))

pwr.plot = ggplot(vals, aes(x = effect.size, y = Power, group = k.type, color = k.type)) +
           geom_point() + 
           geom_line() + 
           geom_hline(yintercept = .8, linetype=2) +
           theme_bw()

pwr.plot
```

Results from either measure of central tendency suggest that your analysis is only sufficiently powered for moderate effect sizes. Can the authors provide some justification that they should expect these larger effect sizes? Especially in light of the discussion they offer of the many reasons why the aggregate picture of disfluency/silence per conversation may be misleafing. 

What about simply looking at the relationship between disfluency rates and planning time, or either and number of variable contexts?

```{r dis_by_sil}
disXsil = ggplot(dat, aes(x = Norm.Overt.Disfluencies, y = Norm.Seconds.Silence, color = Speaker_Number, fill=Speaker_Number)) +
          xlab("# disfluencies") + 
          ylab("Silence (s)") +
          geom_smooth(method = lm, se=T, alpha=0.1) + 
          ylim(0, 80) +
          theme_bw()

disXsil

disXvc = ggplot(dat, aes(y = Norm.Overt.Disfluencies, x = Norm.Variable.Contexts, color = Speaker_Number, fill = Speaker_Number)) +
          ylab("# disfluencies") + 
          xlab("# variable structures") +
          geom_smooth(method = lm, se=T, alpha=0.1) + 
          theme_bw()

disXvc

silXvc = ggplot(dat, aes(y = Norm.Seconds.Silence, x = Norm.Variable.Contexts, color = Speaker_Number, fill = Speaker_Number)) +
          ylab("Silence (s)") + 
          xlab("# variable structures") +
          geom_smooth(method = lm, se=T, alpha=0.1) + 
          ylim(0, 80) +
          theme_bw()

silXvc
```

Of note is the fact that the lines (a) have large error bars (indicative of the small sample sizes) and (b) do not follow any single pattern (i.e., some are positive, some null, some negative).

An attempt at replicating the results of the rmcorr analysis
```{r rmcorr_rep}
# Disfluencies and variable contexts (no control for sample size)
test.dis = rmcorr(Speaker_Number, Total.Variable.Contexts, Total.Overt.Disfluencies, dat[dat$Speaker_Number %in% freq.spkrs,])

plot(test.dis, overall=T, overall.lwd = 3, overall.col = "black", overall.lty = 1, xlab = "Number of variable contexts", ylab = "Number of overt disfluencies", main = "Non-normalized values")

# Planning time and variable contexts (no control for sample size)
test.sil = rmcorr(Speaker_Number, Total.Variable.Contexts, Total.Seconds.Silence, dat)

plot(test.sil, overall=T, overall.lwd = 3, overall.col = "black", overall.lty = 1, xlab = "Number of variable contexts", ylab = "Seconds of silence", main = "Non-normalized values")

```

So without normalizing per 100 words, we find an effect opposite to what is reported in the paper. Now we try with normalized values. 

```{r rmcorr_norm}
# Normalized disfluencies
test.dis.norm = rmcorr(Speaker_Number, Norm.Variable.Contexts, Norm.Overt.Disfluencies, dat)

plot(test.dis.norm, overall=T, overall.lwd = 3, overall.col = "black", overall.lty = 1, xlab = "Number of variable contexts", ylab = "Number of overt disfluencies", main = "Normalized values (per 100 words)")

# Noramlized seconds silence
test.sil.norm = rmcorr(Speaker_Number, Norm.Variable.Contexts, Norm.Seconds.Silence, dat)

plot(test.sil.norm, overall=T, overall.lwd = 3, overall.col = "black", overall.lty = 1, xlab = "Number of variable contexts", ylab = "Seconds of silence", main = "Normalized values")


##################################
# Only speakers >= 5 conversations
##################################
# Normalized disfluencies
test.dis.norm.freq = rmcorr(Speaker_Number, Norm.Variable.Contexts, Norm.Overt.Disfluencies, dat[dat$Speaker_Number %in% freq.spkrs,])

plot(test.dis.norm.freq, overall=T, overall.lwd = 3, overall.col = "black", overall.lty = 1, xlab = "Number of variable contexts", ylab = "Number of overt disfluencies", main = "Normalized values (per 100 words; conversations > 5)")

# Noramlized seconds silence
test.sil.norm.freq = rmcorr(Speaker_Number, Norm.Variable.Contexts, Norm.Seconds.Silence, dat[dat$Speaker_Number %in% freq.spkrs,])

plot(test.sil.norm.freq, overall=T, overall.lwd = 3, overall.col = "black", overall.lty = 1, xlab = "Number of variable contexts", ylab = "Seconds of silence", main = "Normalized values (conversations > 5)")

```

These plots seem to replicate what is presented in the text. Even when we remove the speakers that contribute 1, 2, or 3 data points only, the effects weaken (i.e., approach null slopes).

Now we try some additional modeling techniques:

Poisson
```{r poisson}
pois.mod = glmer(Total.Overt.Disfluencies ~ Total.Variable.Contexts*Total.Words + (1|Speaker_Number), data=dat, family="poisson")

summary(pois.mod)
qqnorm(resid(pois.mod)) 

plot(allEffects(pois.mod), rescale.axis = F, rug=F)
```

Seems to be compatible: however, there does appear to be an effect of size of the conversation: shorter conversations show (slightly) increasing trends, which gradually flip direction into a strong negative trend. 

Looking now for non-linearities
```{r gamm_poisson}
gam.pois = gam(Total.Overt.Disfluencies ~ te(Total.Variable.Contexts, Total.Words) + s(Speaker_Number, bs="re"), data=dat, family="poisson")

vis.gam(gam.pois, plot.type="contour", color = "heat")

vis.gam(gam.pois, plot.type = "persp", color = "heat", theta=45)

plotRGL(sm(getViz(gam.pois), 1), residuals=F)
```

Allowing for the non-linearities reveals some new patterns. For example, for smaller conversations, the number of disfluencies decreases as a function of the number of contexts, but then rebounds into a positive slope in the upper ranges of that variable. For the longest conversations, there is a sharp positive slope for the lower range of numbers of variable contexts, which levels out. All of this to say that the story is a bit more complex. 

What happens if we look at the normalized freqs?
```{r gamm_poisson_norm}
# Check distributional assumptions of the DV
boxcox(Norm.Overt.Disfluencies ~ 1, data = dat) # no tranformation needed

# No need for the interaction with Total.Words as we have normalized the frequencies
gam.pois.norm = gam(Norm.Overt.Disfluencies ~ s(Norm.Variable.Contexts) + s(Speaker_Number, bs="re"), data=dat)

# Plot the results
plot(gam.pois.norm, sel=1, shade=T)
abline(h=0)

# And check the summary
summary(gam.pois.norm)
```

Here we see an effect very similar to the one reported in the paper. Thus, it appears that the manner in which the sample sizes are controlled for influences the relationship between the variables of interest. The negative trend always emerges, but some of the effects at the extremes are rendered invisible by this approach. When the frequencies are modeled as interacting with the sample size, some (non-linear) trends emerge, but only for the smallest and largest sample sizes. 

Now for seconds of silence.

```{r seconds_model}
# Check distributional assumptions of the DV
boxcox(lm(dat$Total.Seconds.Silence~1)) # indicates a log transform (lambda ~ 0)

# Transform response
dat$tSecs = log(dat$Total.Seconds.Silence)

sil.mod = lmer(tSecs ~ Total.Variable.Contexts*Total.Words + (1|Speaker_Number), data=dat)

anova(sil.mod)

plot(allEffects(sil.mod))
```

Here, see a reverse sort of pattern (i.e., the effect reported in the paper holds for the smaller transcripts but not for the larger ones).

But if we look at the normalized values only:
```{r seconds_mode_norml}
# Check distributional assumptions of the DV
boxcox(lm(dat$Norm.Seconds.Silence~1)) # indicates a log transform (lambda ~ 0)

# Transform response
dat$tSecsNorm = log(dat$Norm.Seconds.Silence)

sil.mod = lmer(tSecsNorm ~ Norm.Variable.Contexts + (1|Speaker_Number), data=dat)

anova(sil.mod)

plot(allEffects(sil.mod))
```

And now, what happens if we allow for some non-linearities?
```{r seconds_model_gamm}
sil.mod.gamm = gam(tSecs ~ te(Total.Variable.Contexts, Total.Words) + s(Speaker_Number, bs="re"), data = dat)

summary(sil.mod.gamm)

vis.gam(sil.mod.gamm, plot.type="contour", color = "heat")

vis.gam(sil.mod.gamm, plot.type = "persp", color = "heat", theta=45)

plotRGL(sm(getViz(sil.mod.gamm), 1), residuals=F)
```

So there is something going on here, but the confidence planes suggest that it is really not of much interest. Generally, the effect of variable contexts is positive for the smaller conversations and null or slightly negative for the longer conversations (almost identical to what we see in the linear model above). 

...and normalized values?
```{r seconds_model_gamm_norm}
sil.mod.gamm.norm = gam(tSecsNorm ~ s(Norm.Variable.Contexts) + s(Speaker_Number, bs="re"), data = dat)

summary(sil.mod.gamm.norm)

plot(sil.mod.gamm.norm, sel=1, shade=T)
abline(h=0)
```

Here we replicate the positive trend reported in the paper, with very little curvature. 

I leave it at this. It appears that the patterns you report are truly present in this dataset. However, the issues with power (along with myriad other questions concerning the nature of variability in situ -- is there always a choice where there "could" be one?, the distributions of disfluencies/pauses relative to specific types of structures in real time, etc.) still leave room for some skepticism.  